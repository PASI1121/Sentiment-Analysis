{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbb06eb-e2fc-4c5e-9b6c-34821fd3db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pasindu\\desktop\\sentiment-analysis\\env\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasindu\\desktop\\sentiment-analysis\\env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 838.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 838.9 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 853.0 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 853.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 780.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 791.2 kB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl (269 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.1 regex-2024.7.24\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "82959e46-9589-49e6-a76c-86d59348234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\pasindu\\desktop\\sentiment-analysis\\env\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pasindu\\desktop\\sentiment-analysis\\env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.5.1-cp312-cp312-win_amd64.whl (10.9 MB)\n",
      "Using cached scipy-1.14.0-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.1 scipy-1.14.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a4b143ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "144547c6-d6f0-4310-9cdc-dee6071e523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Check out this link: https://example.com! It's amazing, right? 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "09b76bec-175e-4324-9fc3-eb9574152a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pasindu\\Desktop\\Sentiment-Analysis\\env\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../static/model/model.pickle','rb')as f:\n",
    "    model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9a0a3a9b-6301-432e-8e4e-9337f4ccf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('../static/model/vocabulary.txt',header=None)\n",
    "tokens= vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "15a92427-83dd-4b35-9596-99c272d2af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../static/model/corpora/stopwords/english','r') as file:\n",
    "    sw = file.read().splitlines()\n",
    "string.punctuation\n",
    "\n",
    "punctuation_pattern = r'[^\\w\\s]' \n",
    "number_pattern = r'\\d+'\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "661e0235-6f13-4c53-b682-62617709573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocessing(text):\n",
    "    data = pd.DataFrame([text],columns=['tweet'])\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',x,flags=re.MULTILINE) for x in x.split()))\n",
    "    data['tweet'] = data['tweet'].str.replace(punctuation_pattern, '', regex=True)\n",
    "    data['tweet'] = data['tweet'].str.replace(number_pattern, '', regex=True)\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(x for x in x.split() if x not in sw))\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(stemmer.stem(x) for x in x.split()))\n",
    "    return data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a97bbf72-4847-4e7a-9977-e225bbde6db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    check link amaz right\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_txt = preprocessing(txt)\n",
    "preprocessed_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "781e96c0-71eb-42c9-97ff-19e381609561",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('../static/model/vocabulary.txt',header=None)\n",
    "tokens= vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb140d40-7a25-4469-992b-6a86333dfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds,vocabulary):\n",
    "    vectorized_lst=[]\n",
    "    for sentence in ds:\n",
    "        sentence_lst = np.zeros(len(vocabulary))\n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_lst[i] =1\n",
    "\n",
    "        vectorized_lst.append(sentence_lst)\n",
    "\n",
    "    vectorized_lst_new = np.asarray(vectorized_lst,dtype=np.float32)\n",
    "    if len(ds)==1:\n",
    "        return vectorized_lst_new[0]\n",
    "    return vectorized_lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "21194e5f-3ac3-49d1-8d97-6662669ecd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_txt=vectorizer(preprocessed_txt,tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8a2c31c1-ad7e-4be8-bd72-6127d86b46f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prediction(vectorized_txt):\n",
    "    prediction = model.predict([vectorized_txt])\n",
    "    if prediction == 1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "get_prediction(vectorized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d42bd-4eeb-44b9-b7f1-4dd08b2d5833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
